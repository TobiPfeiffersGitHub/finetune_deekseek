{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promt = \"\"\"\n",
    "You are a helpful customer support assistant. Read the inquiry and generate a professional response.\n",
    "\n",
    "Customer: \"{}\"\n",
    "\n",
    "Response:\n",
    "<think>Understand the request and generate a courteous, helpful reply.\n",
    "<response>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tobinho1234/deepseek-lora-customer-support\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a few prompts\n",
    "sample_inputs = [\n",
    "    \"Hi, I ordered the wrong item. Can I exchange it?\",\n",
    "    \"I never got a confirmation email after purchase.\",\n",
    "    \"How can I reset my password?\",\n",
    "]\n",
    "\n",
    "for query in sample_inputs:\n",
    "    prompt = system_promt.format(query)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    print(f\"User: {query}\\nResponse:\\n{tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
